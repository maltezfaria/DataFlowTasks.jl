<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tiled Cholesky Factorization · DataFlowTasks.jl</title><meta name="title" content="Tiled Cholesky Factorization · DataFlowTasks.jl"/><meta property="og:title" content="Tiled Cholesky Factorization · DataFlowTasks.jl"/><meta property="twitter:title" content="Tiled Cholesky Factorization · DataFlowTasks.jl"/><meta name="description" content="Documentation for DataFlowTasks.jl."/><meta property="og:description" content="Documentation for DataFlowTasks.jl."/><meta property="twitter:description" content="Documentation for DataFlowTasks.jl."/><meta property="og:url" content="https://maltezfaria.github.io/DataFlowTasks.jl/examples/cholesky/cholesky/"/><meta property="twitter:url" content="https://maltezfaria.github.io/DataFlowTasks.jl/examples/cholesky/cholesky/"/><link rel="canonical" href="https://maltezfaria.github.io/DataFlowTasks.jl/examples/cholesky/cholesky/"/><script data-outdated-warner src="../../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../search_index.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../../">DataFlowTasks.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../../">Getting started</a></li><li><a class="tocitem" href="../../../profiling/">Debugging &amp; Profiling</a></li><li><span class="tocitem">Examples</span><ul><li class="is-active"><a class="tocitem" href>Tiled Cholesky Factorization</a><ul class="internal"><li><a class="tocitem" href="#Sequential-implementation"><span>Sequential implementation</span></a></li><li><a class="tocitem" href="#Parallel-implementation"><span>Parallel implementation</span></a></li><li><a class="tocitem" href="#Debugging-and-Profiling"><span>Debugging and Profiling</span></a></li><li class="toplevel"><a class="tocitem" href="#Performances"><span>Performances</span></a></li></ul></li><li><a class="tocitem" href="../../blur-roberts/blur-roberts/">Blur &amp; Roberts image filters</a></li><li><a class="tocitem" href="../../lcs/lcs/">Longest Common Subsequence</a></li><li><a class="tocitem" href="../../sort/sort/">Merge sort</a></li><li><a class="tocitem" href="../../hardware/">Hardware information</a></li></ul></li><li><a class="tocitem" href="../../../troubleshooting/">Troubleshooting</a></li><li><a class="tocitem" href="../../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Tiled Cholesky Factorization</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tiled Cholesky Factorization</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/maltezfaria/DataFlowTasks.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/maltezfaria/DataFlowTasks.jl/blob/main/docs/src/examples/cholesky/cholesky.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="tiledcholesky-section"><a class="docs-heading-anchor" href="#tiledcholesky-section">Tiled Cholesky Factorization</a><a id="tiledcholesky-section-1"></a><a class="docs-heading-anchor-permalink" href="#tiledcholesky-section" title="Permalink"></a></h1><p><a href="../cholesky.ipynb"><img src="https://img.shields.io/badge/download-ipynb-blue" alt="ipynb"/></a> <a href="https://nbviewer.jupyter.org/github/maltezfaria/DataFlowTasks.jl/blob/gh-pages/dev/examples/cholesky/cholesky.ipynb"><img src="https://img.shields.io/badge/show-nbviewer-blue.svg" alt="nbviewer"/></a></p><p>We illustrate here the use of <code>DataFlowTasks</code> to parallelize a tiled Cholesky factorization. The implementation shown here is delibarately made as simple and self-contained as possible; yet, as we shall see when comparing to <em>OpenBLAS</em>, it is already quite performant!</p><p>The Cholesky factorization algorithm takes a symmetric positive definite matrix <span>$A$</span> and finds a lower triangular matrix <span>$L$</span> such that <span>$A = LLᵀ$</span>. The tiled version of this algorithm decomposes the matrix <span>$A$</span> into tiles (of even sizes, in this simplified version). At each step of the algorithm, we do a Cholesky factorization on the diagonal tile, use a triangular solve to update all of the tiles at the right of the diagonal tile, and finally update all the tiles of the submatrix with a schur complement.</p><p>If we have a matrix <span>$A$</span> decomposed in <span>$n \times n$</span> tiles, then the algorithm will have <span>$n$</span> steps. The <span>$i$</span>-th step (with <span>$i \in [1:n]$</span>) performs:</p><ul><li> <span>$1$</span> cholesky factorization of the <span>$(i,i)$</span> tile,</li><li> <span>$(i-1)$</span> triangular solves (one for each tile in the <span>$i$</span>-th row of the upper triangular matrix),</li><li> <span>$i(i-1)/2$</span> matrix multiplications to update the submatrix.</li></ul><p>These are the basic operations on tiles, which we are going to spawn in separate tasks in the parallel implementation. Accounting for all iterations, this makes a total of <span>$\mathcal{O}(n^3)$</span> such tasks, decomposed as:</p><ul><li> <span>$\mathcal{O}(n)$</span> cholesky factorizations,</li><li> <span>$\mathcal{O}(n^2)$</span> triangular solves,</li><li> <span>$\mathcal{O}(n^3)$</span> matrix multiplications.</li></ul><p>The following image illustrates the 2nd step of the algorithm:</p><p><img src="https://github.com/maltezfaria/DataFlowTasks.jl/blob/1d101c3d738bcb35a93ed4c9db60a2ad66886fa5/docs/src/examples/cholesky/Cholesky_2ndStep.png?raw=true" alt/></p><h2 id="Sequential-implementation"><a class="docs-heading-anchor" href="#Sequential-implementation">Sequential implementation</a><a id="Sequential-implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Sequential-implementation" title="Permalink"></a></h2><p>A sequential tiled factorization algorithm can be implemented as:</p><pre><code class="language-julia hljs">using LinearAlgebra

tilerange(ti, ts) = (ti-1)*ts+1:ti*ts

function cholesky_tiled!(A, ts)
    m = size(A, 1); @assert m==size(A, 2)
    m%ts != 0 &amp;&amp; error(&quot;Tilesize doesn&#39;t fit the matrix&quot;)
    n = m÷ts  # number of tiles in each dimension

    T = [view(A, tilerange(i, ts), tilerange(j, ts)) for i in 1:n, j in 1:n]

    for i in 1:n
        # Diagonal cholesky serial factorization
        cholesky!(T[i,i])

        # Left tiles update
        U = UpperTriangular(T[i,i])
        for j in i+1:n
            ldiv!(U&#39;, T[i,j])
        end

        # Submatrix update
        for j in i+1:n
            for k in j:n
                mul!(T[j,k], T[i,j]&#39;, T[i,k], -1, 1)
            end
        end
    end

    # Construct the factorized object
    return Cholesky(A, &#39;U&#39;, zero(LinearAlgebra.BlasInt))
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">cholesky_tiled! (generic function with 1 method)</code></pre><p>Let us build a small test case to check the correctness of the factorization. Here we divide a matrix of size 4096×4096 in 8×8 tiles of size 512×512:</p><pre><code class="language-julia hljs">n  = 4096
ts = 512
A = rand(n, n)
A = (A + adjoint(A))/2
A = A + n*I;</code></pre><p>and the results seem to be correct:</p><pre><code class="language-julia hljs">F = cholesky_tiled!(copy(A), ts)

# Check results
err = norm(F.L*F.U-A,Inf)/max(norm(A),norm(F.L*F.U))
@show err
@assert err &lt; eps(Float64)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">err = 1.7344504839115092e-17</code></pre><h2 id="Parallel-implementation"><a class="docs-heading-anchor" href="#Parallel-implementation">Parallel implementation</a><a id="Parallel-implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Parallel-implementation" title="Permalink"></a></h2><p>In order to parallelize the code with <code>DataFlowTasks.jl</code>, function calls acting on tiles are wrapped within <code>@dspawn</code>, along with annotations describing data access modes. We also give meaningful labels to the tasks, which will help debug and profile the code.</p><pre><code class="language-julia hljs">using DataFlowTasks

function cholesky_dft!(A, ts)
    m = size(A, 1); @assert m==size(A, 2)
    m%ts != 0 &amp;&amp; error(&quot;Tilesize doesn&#39;t fit the matrix&quot;)
    n = m÷ts  # number of tiles in each dimension

    T = [view(A, tilerange(i, ts), tilerange(j, ts)) for i in 1:n, j in 1:n]

    for i in 1:n
        # Diagonal cholesky serial factorization
        @dspawn cholesky!(@RW(T[i,i])) label=&quot;chol ($i,$i)&quot;

        # Left tiles update
        U = UpperTriangular(T[i,i])
        for j in i+1:n
            @dspawn ldiv!(@R(U)&#39;, @RW(T[i,j])) label=&quot;ldiv ($i,$j)&quot;
        end

        # Submatrix update
        for j in i+1:n
            for k in j:n
                @dspawn schur_complement!(@RW(T[j,k]), @R(T[i,j])&#39;, @R(T[i,k])) label=&quot;schur ($j,$k)&quot;
            end
        end
    end

    # Construct the factorized object
    r = @dspawn Cholesky(@R(A), &#39;U&#39;, zero(LinearAlgebra.BlasInt)) label=&quot;result&quot;
    return fetch(r)
end

schur_complement!(C, A, B) = mul!(C, A, B, -1, 1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">schur_complement! (generic function with 1 method)</code></pre><p>Again, let us check the correctness of the result:</p><pre><code class="language-julia hljs">F = cholesky_dft!(copy(A), ts)

# Check results
err = norm(F.L*F.U-A,Inf)/max(norm(A),norm(F.L*F.U))
@show err
@assert err &lt; eps(Float64)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">err = 1.7344504839115092e-17</code></pre><h2 id="Debugging-and-Profiling"><a class="docs-heading-anchor" href="#Debugging-and-Profiling">Debugging and Profiling</a><a id="Debugging-and-Profiling-1"></a><a class="docs-heading-anchor-permalink" href="#Debugging-and-Profiling" title="Permalink"></a></h2><p>Let us now check what happens during a parallel run of our cholesky factorization. Thanks to the test above, the code is now compiled. Let&#39;s re-run it and collect meaningful profiling information:</p><pre><code class="language-julia hljs"># Clean profiling environment
GC.gc()

# Real workload to be analysed
Ac = copy(A)
log_info = DataFlowTasks.@log cholesky_dft!(Ac, ts)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">LogInfo with 121 logged tasks
</code></pre><p>The number of tasks being <span>$\mathcal{O}(n^3)$</span>, we can see how quickly the DAG complexity increases (even though the test case only has 8×8 tiles here):</p><pre><code class="language-julia hljs">DataFlowTasks.stack_weakdeps_env!()
using GraphViz
dag = GraphViz.Graph(log_info)</code></pre><img src="52209735.svg" alt="Example block output"/><p>The critical path, highlighted in red, includes all cholesky factorizations of diagonal tiles, as well as the required tasks in between them.</p><p>We can also readily get more details about the performance limiting factors:</p><pre><code class="language-julia hljs">DataFlowTasks.describe(log_info; categories=[&quot;chol&quot;, &quot;ldiv&quot;, &quot;schur&quot;])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">• Elapsed time           : 0.259
  ├─ Critical Path       : 0.223
  ╰─ No-Wait             : 0.178

• Run time               : 2.074
  ├─ Computing           :   1.427
  │  ├─ chol             :     0.051
  │  ├─ ldiv             :     0.405
  │  ├─ schur            :     0.971
  │  ╰─ unlabeled        :     0.000
  ├─ Task Insertion      :   0.003
  ╰─ Other (idle)        :   0.644</code></pre><p>and, at the price of loading <code>Makie</code>, display these in a more convenient profile plot:</p><pre><code class="language-julia hljs">using CairoMakie # or GLMakie in order to have more interactivity
trace = plot(log_info; categories=[&quot;chol&quot;, &quot;ldiv&quot;, &quot;schur&quot;])</code></pre><img src="262f02f2.png" alt="Example block output"/><p>The overhead incurred by <code>DataFlowTasks</code> seems relatively small here: the time taken inserting tasks is barely measurable, and the scheduling did not lead to threads waiting idly for too long. This is confirmed by the bottom middle plot, showing a measured wall clock time not too much longer than the lower bound obtained when suppressing idle time.</p><p>The &quot;Computing time: breakdown by category&quot; plot seems to indicate that the matrix multiplications performed in the &quot;Schur&quot; tasks account for the majority of the computing time. This routine is probably already quite fast since it simply calls our <em>BLAS</em> library for a <em>matrix-matrix</em> product; it would be interesting, however, to see how <a href="https://github.com/JuliaSIMD/LoopVectorization.jl"><code>LoopVectorization.jl</code></a> fares here!</p><h1 id="Performances"><a class="docs-heading-anchor" href="#Performances">Performances</a><a id="Performances-1"></a><a class="docs-heading-anchor-permalink" href="#Performances" title="Permalink"></a></h1><p>To benchmark the performance, we will compare our implementation to the one provided by our system&#39;s BLAS library. We will use <a href="https://www.openblas.net">OpenBlas</a> here because it is the default BLAS library shipped with <em>Julia</em>, but if you have access to Intel&#39;s MKL, you should probably give it a try! Here is the benchmark:</p><pre><code class="language-julia hljs">using BenchmarkTools
BenchmarkTools.DEFAULT_PARAMETERS.seconds = 1

# n × n symmetric positive definite matrix
function spd_matrix(n)
    A = rand(n, n)
    A = (A + adjoint(A))/2
    return A + n*I
end

function bench_blas(n)
    nt = Threads.nthreads()
    BLAS.set_num_threads(nt)
    return @belapsed cholesky!(A) setup=(A=spd_matrix($n)) evals=1
end

function bench_tiled(n;tilesize=256)
    BLAS.set_num_threads(1)
    return @belapsed cholesky_tiled!(A, $tilesize) setup=(A=spd_matrix($n)) evals=1
end

function bench_dft(n;tilesize=256)
    BLAS.set_num_threads(1)
    return @belapsed cholesky_dft!(A, $tilesize) setup=(A=spd_matrix($n)) evals=1
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">bench_dft (generic function with 1 method)</code></pre><p>Let us compare the performances of the default <em>BLAS</em>  library and ours for various matrix sizes, and plot the results:</p><pre><code class="language-julia hljs">BLAS.get_config()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">LinearAlgebra.BLAS.LBTConfig
Libraries: 
└ [ILP64] libopenblas64_.so</code></pre><pre><code class="language-julia hljs">nsizes = 512 .* (1:1:8)
tblas  = map(bench_blas, nsizes)
tdft   = map(bench_dft, nsizes)
Gflops =  map(n-&gt;(1/3*n^3 + 1/2*n^2)/10^9, nsizes)

fig = Figure()
ax  = Axis(fig[1,1], xlabel=&quot;Matrix size&quot;, ylabel=&quot;Time (s)&quot;)
scatterlines!(ax, nsizes, tblas, label= &quot;OpenBLAS&quot;, linewidth=2)
scatterlines!(ax, nsizes, tdft, label=&quot;DFT&quot;, linewidth=2)
axislegend(position=:lt)
ax  = Axis(fig[1,2], xlabel=&quot;Matrix size&quot;, ylabel=&quot;GFlops/second&quot;)
scatterlines!(ax, nsizes, Gflops ./ tblas, label= &quot;OpenBLAS&quot;, linewidth=2)
scatterlines!(ax, nsizes, Gflops ./ tdft, label=&quot;DFT&quot;, linewidth=2)
axislegend(position=:lt)
Label(fig[0,:],  &quot;Cholesky factorization on $(Threads.nthreads()) threads&quot;, fontsize=20)
fig</code></pre><img src="bb257358.png" alt="Example block output"/><p>We see that, despite the simplicity of the implementation, the parallel version performs <em>in par</em> with the default <em>BLAS</em> library for the matrix sizes considered! For very large matrices, further optmizations are probably necessary to take into account the memory hierarchy of the machine. Finally, here is the observed speedup compared to a sequential tiled implementation and a matrix of size <span>$n=4096$</span>:</p><pre><code class="language-julia hljs">(;
 threads = Threads.nthreads(),
 speedup = bench_tiled(4096) / bench_dft(4096),
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(threads = 8, speedup = 2.492298765600089)</code></pre><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../../profiling/">« Debugging &amp; Profiling</a><a class="docs-footer-nextpage" href="../../blur-roberts/blur-roberts/">Blur &amp; Roberts image filters »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.1 on <span class="colophon-date" title="Friday 7 June 2024 12:13">Friday 7 June 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
